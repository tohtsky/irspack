{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "In this tutorial, we first demonstrate how `P3alphaRecommender`'s performance can be optimized\n",
    "by [optuna](https://github.com/optuna/optuna)-backed `P3alphaOptimizer`.\n",
    "\n",
    "Then, by further splitting the ground-truth interaction into tran, validation and test ones,\n",
    "we compare several recommenders' performance optimized on the validation set and measured on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from irspack.dataset.movielens import MovieLens1MDataManager\n",
    "from irspack.recommenders import P3alphaRecommender\n",
    "from irspack.optimizers import P3alphaOptimizer\n",
    "from irspack.split import rowwise_train_test_split\n",
    "from irspack.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comptutation might be heavy, so we use multiple threads to speed up the training and evaluation.\n",
    "\n",
    "You can tell our algorithms to use mutiple threads whenever possible by setting ``IRSPACK_NUM_THREADS_DEFAULT`` environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"IRSPACK_NUM_THREADS_DEFAULT\"] = \"8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the ML1M dataset again.\n",
    "\n",
    "We again prepare the sparse matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = MovieLens1MDataManager()\n",
    "\n",
    "df = loader.read_interaction()\n",
    "\n",
    "movies = loader.read_item_info()\n",
    "movies.head()\n",
    "\n",
    "unique_user_ids, user_index = np.unique(df.userId, return_inverse=True)\n",
    "unique_movie_ids, movie_index = np.unique(df.movieId, return_inverse=True)\n",
    "\n",
    "movie_id_vs_movie_index = { mid: i for i, mid in enumerate(unique_movie_ids)}\n",
    "\n",
    "X = sps.csr_matrix(\n",
    "    (\n",
    "        np.ones(df.shape[0]),\n",
    "        ( user_index, movie_index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split scheme 2. Hold-out for partial users.\n",
    "\n",
    "To perform the hyperparameter optimization, we have to repeatedly measure the accuracy metrics on the validation set. As mentioned in the previous tutorial, doing this for all users is time-comsuming (often heavier than the recommender's learning process), so we truncate this subset as follows:\n",
    "\n",
    "1. First split **users** into \"train\", \"validation\" (and \"test\") ones.\n",
    "1. For train users, feed all their interactions into the recommender. For validation (test) users, hold-out part of their interaction for the validation (\"prediction\" part), and feed the rest (\"learning\" part) into the recommender.\n",
    "1. After the fit, ask the recommender to output the score only for validation (test) users, and see how it ranks these held-out interactions for the validation (test) users.\n",
    "\n",
    "![Perform hold out for part of users.](./split2.png \"split1\")\n",
    "\n",
    "Although we have prepared another function to do this procedure, let us first do this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split users into train and validation users.\n",
    "\n",
    "X_train_user, X_valid_user = train_test_split(X, test_size=.4, random_state=0)\n",
    "\n",
    "# Split the validation users' interaction into learning 50% and predcition 50%.\n",
    "\n",
    "X_valid_learn, X_valid_predict = rowwise_train_test_split(\n",
    "    X_valid_user, test_ratio=.5, random_seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the evaluator and optimize the validation metric\n",
    "\n",
    "As illustrated above, we will use \n",
    "\n",
    " * Train users' all interactions (``X_train_user``)\n",
    " * Validation users' 50% interaction (``X_valid_learn``)\n",
    " \n",
    "as the recommender's training resource, and validation users' rest interaction (``X_valid_predict``) as the held-out ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_learn = sps.vstack([X_train_user, X_valid_learn])\n",
    "evaluator = Evaluator(X_valid_predict, offset=X_train_user.shape[0], target_metric='ndcg', cutoff=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``offset`` parameter specifies where the validation user block begins (where the train user block ends).\n",
    "\n",
    "Now to start the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Truncating the stderr output as it's a bit lengthy to show in the documentation.\n",
    "    # When you run this note book, you don't have to truncate it.\n",
    "    from irspack.utils.default_logger import disable_default_handler\n",
    "    import optuna.logging\n",
    "    disable_default_handler()\n",
    "    optuna.logging.disable_default_handler() \n",
    "    \n",
    "optimizer = P3alphaOptimizer(X_train_val_learn, evaluator)\n",
    "best_params, validation_results = optimizer.optimize(random_seed=0, n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best `ndcg@20` value is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5190536602989886"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_results.ndcg.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which has been obtained by using these hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 5.063313369473015e-07, 'top_k': 161, 'normalize_weight': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the default argument of ``P3alphaRecommdner`` (which has been used so far)\n",
    "attains `ndcg@20` = 0.404. So this is indeed a significant improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4041442008349765"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_default = P3alphaRecommender(X_train_val_learn).learn()\n",
    "evaluator.get_score(rec_default)['ndcg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the recommender's output again\n",
    "\n",
    "Let us check how our recommender has evolved from the first tutorial. We consider the same setting (a new user has watched \"Toy Story\"), but fit the \n",
    "recommender using the obtained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_tuned = P3alphaRecommender(X, **best_params).learn()\n",
    "\n",
    "from irspack.utils.id_mapping import IDMappedRecommender\n",
    "id_mapped_rec = IDMappedRecommender(\n",
    "    rec_tuned, user_ids=unique_user_ids, item_ids=unique_movie_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Groundhog Day (1993)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>Shakespeare in Love (1998)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>Toy Story 2 (1999)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>Back to the Future (1985)</td>\n",
       "      <td>Comedy|Sci-Fi</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>Saving Private Ryan (1998)</td>\n",
       "      <td>Action|Drama|War</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Babe (1995)</td>\n",
       "      <td>Children's|Comedy|Drama</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Forrest Gump (1994)</td>\n",
       "      <td>Comedy|Romance|War</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>Princess Bride, The (1987)</td>\n",
       "      <td>Action|Adventure|Comedy|Romance</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>Aladdin (1992)</td>\n",
       "      <td>Animation|Children's|Comedy|Musical</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title                               genres  \\\n",
       "movieId                                                                    \n",
       "1265           Groundhog Day (1993)                       Comedy|Romance   \n",
       "2396     Shakespeare in Love (1998)                       Comedy|Romance   \n",
       "3114             Toy Story 2 (1999)          Animation|Children's|Comedy   \n",
       "1270      Back to the Future (1985)                        Comedy|Sci-Fi   \n",
       "2028     Saving Private Ryan (1998)                     Action|Drama|War   \n",
       "34                      Babe (1995)              Children's|Comedy|Drama   \n",
       "356             Forrest Gump (1994)                   Comedy|Romance|War   \n",
       "2355           Bug's Life, A (1998)          Animation|Children's|Comedy   \n",
       "1197     Princess Bride, The (1987)      Action|Adventure|Comedy|Romance   \n",
       "588                  Aladdin (1992)  Animation|Children's|Comedy|Musical   \n",
       "\n",
       "         release_year  \n",
       "movieId                \n",
       "1265             1993  \n",
       "2396             1998  \n",
       "3114             1999  \n",
       "1270             1985  \n",
       "2028             1998  \n",
       "34               1995  \n",
       "356              1994  \n",
       "2355             1998  \n",
       "1197             1987  \n",
       "588              1992  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toystory_id = 1\n",
    "recommended_id_and_score = id_mapped_rec.get_recommendation_for_new_user(\n",
    "    item_ids=[toystory_id], cutoff=10\n",
    ")\n",
    "\n",
    "# Top-10 recommendations\n",
    "movies.reindex([movie_id for movie_id, score in recommended_id_and_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how drastically the recommended contents have changed (increased significance of genre \"Children's\" and disapperance of \"Star Wars\" series, etc...).\n",
    "\n",
    "## A train/validation/test split example\n",
    "\n",
    "To rigorously compare the performance of various recommender algorithms,\n",
    "we should measure the final score against the **test** dataset, not the validation set,\n",
    "and it is straightforward now.\n",
    "\n",
    "To begin with, we have prepared a function called ``split_dataframe_partial_user_holdout`` which\n",
    "splits the users in the original dataframe into train/validation/test users,\n",
    "holding out partial interaction for validation/test user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <irspack.split.random.UserTrainTestInteractionPair at 0x7fa11bdffb50>,\n",
       " 'val': <irspack.split.random.UserTrainTestInteractionPair at 0x7fa11bdff410>,\n",
       " 'test': <irspack.split.random.UserTrainTestInteractionPair at 0x7fa11c9e8f10>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from irspack.split import split_dataframe_partial_user_holdout\n",
    "\n",
    "dataset, item_ids = split_dataframe_partial_user_holdout(\n",
    "    df, 'userId', 'movieId', val_user_ratio=.3, test_user_ratio=.3,\n",
    "    heldout_ratio_val=.5, heldout_ratio_test=.5\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the returned ``dataset`` is a dictionary which stores train/validation/test-users' interactions as an instance of ``UserTrainTestInteractionPair``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = dataset['train']\n",
    "val_users = dataset['val']\n",
    "test_users = dataset['test']\n",
    "\n",
    "# Concatenate train/validation users into one.\n",
    "train_and_val_users = train_users.concat(val_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1812x3597 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 149202 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_users.X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1812x3597 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 148323 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_users.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1812x3597 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 297525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_users.X_all # which equals val_users.X_train + val_users.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2416x3597 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For train users, there is no \"test\" interaction held out.\n",
    "train_users.X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each recommender algorithm (here ``P3alpha``, ``RP3beta``, ``IALS`` and ``DenseSLIM``), we perform:\n",
    "\n",
    "1. Hyperparameter optimization. During this phase, we will be using train users' all interaction and validation\n",
    "   users' train interaction as the source of learning, and validation users' test interaction as the held-out ground truth.\n",
    "2. Evaluation. During this phase, we will include train/validation users' all interactions\n",
    "   as well as test users' train interaction as the source of learning,\n",
    "   and fit the model using the parameters obtained in the optimization phase.\n",
    "   Then we measure the recommender's performance against **test** users' test interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from irspack.optimizers import DenseSLIMOptimizer, RP3betaOptimizer, IALSOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running IALSOptimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_score=0.42824183005121735:  11%|█         | 55/512 [00:05<00:45,  9.96it/s]\n",
      "valid_score=0.40060602861013855:  15%|█▍        | 75/512 [00:09<00:52,  8.37it/s]\n",
      "valid_score=0.5333345981232733:  14%|█▎        | 70/512 [00:02<00:17, 25.95it/s]\n",
      "valid_score=0.4565719212930136:   7%|▋         | 35/512 [00:02<00:40, 11.79it/s] \n",
      "valid_score=0.48549781083236443:   8%|▊         | 40/512 [00:02<00:31, 14.81it/s]\n",
      "valid_score=0.5327950253455304:  10%|▉         | 50/512 [00:01<00:17, 25.96it/s]\n",
      "valid_score=0.4767790510450989:   7%|▋         | 35/512 [00:02<00:38, 12.30it/s] \n",
      "valid_score=0.4078509330631947:   1%|          | 5/512 [00:00<00:34, 14.61it/s]\n",
      "valid_score=0.4372874728149235:   1%|          | 5/512 [00:00<00:53,  9.40it/s]\n",
      "valid_score=0.4217175292017891:   1%|          | 5/512 [00:00<00:46, 10.97it/s]\n",
      "valid_score=0.44462285171408994:   1%|          | 5/512 [00:00<00:14, 35.51it/s]\n",
      "valid_score=0.5299876125954996:   9%|▉         | 45/512 [00:01<00:17, 27.08it/s]\n",
      "valid_score=0.5338013123443506:  14%|█▎        | 70/512 [00:03<00:19, 22.57it/s]\n",
      "valid_score=0.5273331792612119:   7%|▋         | 35/512 [00:01<00:21, 22.01it/s]\n",
      "valid_score=0.5286246823761697:   7%|▋         | 35/512 [00:01<00:22, 21.25it/s]\n",
      "valid_score=0.5264078529929245:   7%|▋         | 35/512 [00:01<00:22, 21.03it/s]\n",
      "valid_score=0.44400466852940296:   1%|          | 5/512 [00:00<00:14, 36.19it/s]\n",
      "valid_score=0.5306979586892263:   8%|▊         | 40/512 [00:01<00:16, 27.96it/s]\n",
      "valid_score=0.511043342238914:   7%|▋         | 35/512 [00:02<00:27, 17.24it/s] \n",
      "valid_score=0.4518078505781681:   1%|          | 5/512 [00:00<00:27, 18.14it/s]\n",
      "valid_score=0.533190381872124:  12%|█▏        | 60/512 [00:02<00:19, 23.58it/s] \n",
      "valid_score=0.5330299634914348:  10%|▉         | 50/512 [00:02<00:17, 25.86it/s]\n",
      "valid_score=0.5018637442329386:   1%|          | 5/512 [00:00<00:15, 33.50it/s]\n",
      "valid_score=0.5030662041498615:   1%|          | 5/512 [00:00<00:28, 17.81it/s]\n",
      "valid_score=0.5289883633577348:   2%|▏         | 10/512 [00:00<00:18, 27.63it/s]\n",
      "valid_score=0.49958073757480104:   1%|          | 5/512 [00:00<00:18, 27.99it/s]\n",
      "valid_score=0.5123500965698433:   1%|          | 5/512 [00:00<00:25, 19.92it/s]\n",
      "valid_score=0.5345384958248065:  12%|█▏        | 60/512 [00:02<00:19, 23.70it/s]\n",
      "valid_score=0.49529533153445293:   1%|          | 5/512 [00:00<00:32, 15.41it/s]\n",
      "valid_score=0.5034195494138344:   1%|          | 5/512 [00:00<00:16, 29.90it/s]\n",
      "valid_score=0.37982336414616824:   1%|          | 5/512 [00:00<00:29, 17.36it/s]\n",
      "valid_score=0.5341290557392615:  15%|█▍        | 75/512 [00:03<00:18, 23.60it/s]\n",
      "valid_score=0.5330484337350391:   8%|▊         | 40/512 [00:01<00:19, 24.80it/s]\n",
      "valid_score=0.5169060070657032:   1%|          | 5/512 [00:00<00:25, 20.08it/s]\n",
      "valid_score=0.535784217748701:  11%|█         | 55/512 [00:02<00:19, 23.77it/s] \n",
      "valid_score=0.5350593666909814:   9%|▉         | 45/512 [00:01<00:17, 27.38it/s]\n",
      "valid_score=0.5164001692167451:   1%|          | 5/512 [00:00<00:16, 31.61it/s]\n",
      "valid_score=0.5346230124782899:  15%|█▍        | 75/512 [00:02<00:15, 27.92it/s]/home/tomoki/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1366: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "valid_score=0.5348182715646398:  18%|█▊        | 90/512 [00:03<00:15, 27.65it/s]\n",
      "valid_score=0.5305485694376337:   2%|▏         | 10/512 [00:00<00:17, 28.12it/s]\n",
      "valid_score=0.46877847228072783:   1%|          | 5/512 [00:00<00:15, 33.77it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 32.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running RP3betaOptimizer.\n",
      "Start running P3alphaOptimizer.\n",
      "Start running DenseSLIMOptimizer.\n"
     ]
    }
   ],
   "source": [
    "val_evaluator = Evaluator(\n",
    "    val_users.X_test,\n",
    "    offset=train_users.n_users,\n",
    "    cutoff=20, target_metric=\"ndcg\"\n",
    ")\n",
    "test_evaluator = Evaluator(\n",
    "    test_users.X_test,\n",
    "    offset=train_and_val_users.n_users\n",
    ")\n",
    "test_results = []\n",
    "for optimizer_class in [IALSOptimizer, RP3betaOptimizer, P3alphaOptimizer, DenseSLIMOptimizer]:\n",
    "    print(f'Start running {optimizer_class.__name__}.')\n",
    "    optimizer_ = optimizer_class(\n",
    "        sps.vstack([train_users.X_all, val_users.X_train]),\n",
    "        val_evaluator\n",
    "    )\n",
    "    best_params, validation_results_df = optimizer_.optimize(n_trials=40, random_seed=0)\n",
    "    recommender = optimizer_class.recommender_class(\n",
    "        sps.vstack([train_and_val_users.X_all, test_users.X_train]),\n",
    "        **best_params\n",
    "    ).learn()\n",
    "\n",
    "    test_score = dict(\n",
    "        algorithm=optimizer_class.__name__, \n",
    "        **test_evaluator.get_scores(recommender, cutoffs=[20])\n",
    "    )\n",
    "    test_results.append(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in terms of accuracy-related metrics (recall, ndcg, map), DenseSLIM is a clear winner in this setting.\n",
    "IALS however exibits beter performance with respect to the diversity measure (Gini index and entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>hit@20</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>ndcg@20</th>\n",
       "      <th>map@20</th>\n",
       "      <th>precision@20</th>\n",
       "      <th>gini_index@20</th>\n",
       "      <th>entropy@20</th>\n",
       "      <th>appeared_item@20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IALSOptimizer</td>\n",
       "      <td>0.993929</td>\n",
       "      <td>0.201032</td>\n",
       "      <td>0.549493</td>\n",
       "      <td>0.128710</td>\n",
       "      <td>0.498317</td>\n",
       "      <td>0.914696</td>\n",
       "      <td>5.994704</td>\n",
       "      <td>994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RP3betaOptimizer</td>\n",
       "      <td>0.995033</td>\n",
       "      <td>0.193559</td>\n",
       "      <td>0.537777</td>\n",
       "      <td>0.123390</td>\n",
       "      <td>0.484078</td>\n",
       "      <td>0.949540</td>\n",
       "      <td>5.409107</td>\n",
       "      <td>982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3alphaOptimizer</td>\n",
       "      <td>0.990066</td>\n",
       "      <td>0.186035</td>\n",
       "      <td>0.522004</td>\n",
       "      <td>0.116442</td>\n",
       "      <td>0.469812</td>\n",
       "      <td>0.962653</td>\n",
       "      <td>5.146738</td>\n",
       "      <td>667.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DenseSLIMOptimizer</td>\n",
       "      <td>0.993929</td>\n",
       "      <td>0.210865</td>\n",
       "      <td>0.574984</td>\n",
       "      <td>0.139570</td>\n",
       "      <td>0.520006</td>\n",
       "      <td>0.928136</td>\n",
       "      <td>5.807884</td>\n",
       "      <td>988.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            algorithm    hit@20  recall@20   ndcg@20    map@20  precision@20  \\\n",
       "0       IALSOptimizer  0.993929   0.201032  0.549493  0.128710      0.498317   \n",
       "1    RP3betaOptimizer  0.995033   0.193559  0.537777  0.123390      0.484078   \n",
       "2    P3alphaOptimizer  0.990066   0.186035  0.522004  0.116442      0.469812   \n",
       "3  DenseSLIMOptimizer  0.993929   0.210865  0.574984  0.139570      0.520006   \n",
       "\n",
       "   gini_index@20  entropy@20  appeared_item@20  \n",
       "0       0.914696    5.994704             994.0  \n",
       "1       0.949540    5.409107             982.0  \n",
       "2       0.962653    5.146738             667.0  \n",
       "3       0.928136    5.807884             988.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
