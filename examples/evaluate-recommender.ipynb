{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of recommender systems\n",
    "\n",
    "In this tutorial, we explain how to evaluate recommender systems with implicit feedback\n",
    "by holding out method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "from irspack.dataset.movielens import MovieLens1MDataManager\n",
    "from irspack.recommenders import P3alphaRecommender, TopPopRecommender\n",
    "from irspack.split import rowwise_train_test_split\n",
    "from irspack.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the ML1M dataset again.\n",
    "\n",
    "As in the previous tutorial, we load the rating dataset and construct a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = MovieLens1MDataManager()\n",
    "\n",
    "df = loader.read_interaction()\n",
    "\n",
    "unique_user_ids, user_index = np.unique(df.userId, return_inverse=True)\n",
    "unique_movie_ids, movie_index = np.unique(df.movieId, return_inverse=True)\n",
    "\n",
    "X = sps.csr_matrix(\n",
    "    (\n",
    "        np.ones(df.shape[0]),\n",
    "        ( user_index, movie_index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split scheme 1. Hold-out for all users.\n",
    "\n",
    "To evaluate the performance of a recommender system trained with implicit feedback, the standard method is to hide some subset of the known user-item interactions as a validation set and see how the recommender ranks these hidden groundtruths:\n",
    "\n",
    "![Perform hold out for all users.](./split1.png \"split1\")\n",
    "\n",
    "We have prepared a fast implementaion of such a split (with random selection of these subset) in ``rowwise_train_test_split`` function.\n",
    "\n",
    "This scheme however has a problem regarding the performance because we have to compute the recommendation score for all the users. So in the next tutorial, we will be using a sub-sampled version of this splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = rowwise_train_test_split(X, test_ratio=0.2, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape == X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They sum back to the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6040x3706 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X - (X_train + X_valid) # 0 stored elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no overlap of non-zero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6040x3706 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.multiply(X_valid) # Element-wise multiplication yields 0 stored elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the evaluation metric\n",
    "\n",
    "Now we define the `Evaluator` object, which will measure the performance of various recommender systems based on ``X_valid`` (the meaning of ``offset=0`` will be clarified\n",
    "in the next tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(X_valid, offset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit ``P3alphaRecommender`` using ``X_train``, and compute its accuracy metrics\n",
    "against ``X_valid`` using `evaluator`.\n",
    "\n",
    "Internally, `evaluator` calls the recommender's ``get_score_remove_seen`` method, sorts the score to obtain the rank, and reconciles it with the stored validation interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hit@5', 0.7935430463576159),\n",
       "             ('recall@5', 0.09357329074754922),\n",
       "             ('ndcg@5', 0.39936130766343314),\n",
       "             ('map@5', 0.06671765815079544),\n",
       "             ('gini_index@5', 0.9737846091714528),\n",
       "             ('entropy@5', 4.791553116972738),\n",
       "             ('appeared_item@5', 545.0),\n",
       "             ('hit@10', 0.8966887417218543),\n",
       "             ('recall@10', 0.15261650834932203),\n",
       "             ('ndcg@10', 0.3697677997781827),\n",
       "             ('map@10', 0.09057228932779289),\n",
       "             ('gini_index@10', 0.9613104845194654),\n",
       "             ('entropy@10', 5.199571410742591),\n",
       "             ('appeared_item@10', 771.0)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender = P3alphaRecommender(X_train, top_k=100)\n",
    "recommender.learn()\n",
    "\n",
    "evaluator.get_scores(recommender, cutoffs=[5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with a simple baseline\n",
    "\n",
    "Now that we have a qualitative way to measure the recommenders' performance,\n",
    "we can compare the performance of different algorithms.\n",
    "\n",
    "As a simple baseline, we fit ``TopPopRecommender``, which recommends items\n",
    "with descending order of the popularity in the train set, regardless of the users'\n",
    "watch event history. (But note that already-seen items by a user will not be commended again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hit@5', 0.5319536423841059),\n",
       "             ('recall@5', 0.03990291401117003),\n",
       "             ('ndcg@5', 0.21637600397711307),\n",
       "             ('map@5', 0.02510759708371324),\n",
       "             ('gini_index@5', 0.997223564436407),\n",
       "             ('entropy@5', 2.6085822508131637),\n",
       "             ('appeared_item@5', 43.0),\n",
       "             ('hit@10', 0.6614238410596026),\n",
       "             ('recall@10', 0.06688199444797117),\n",
       "             ('ndcg@10', 0.2004243718788812),\n",
       "             ('map@10', 0.033385869291721104),\n",
       "             ('gini_index@10', 0.9950121246019521),\n",
       "             ('entropy@10', 3.179463712641221),\n",
       "             ('appeared_item@10', 65.0)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toppop_recommender = TopPopRecommender(X_train)\n",
    "toppop_recommender.learn()\n",
    "\n",
    "evaluator.get_scores(toppop_recommender, cutoffs=[5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that `P3alphaRecommender` actually exhibits better accuracy scores compared to rather trivial `TopPopRecommender`.\n",
    "\n",
    "In the next tutorial, we will optimize the recommender's performance using the hold-out method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
